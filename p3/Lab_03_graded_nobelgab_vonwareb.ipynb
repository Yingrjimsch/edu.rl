{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c38aea36",
   "metadata": {},
   "source": [
    "## Week 03: Graded Lab\n",
    "\n",
    "### Lab session\n",
    "\n",
    "Welcome to the third week of our Reinforcement Learning course! In this lab session, we will delve into more advanced reinforcement learning techniques and challenge you to implement and extend what you've learned in the previous two weeks. This graded lab is designed to be more time-consuming and challenging, but it will be a rewarding experience as you tackle complex problems in reinforcement learning.\n",
    "\n",
    "### Educational Objectives\n",
    "\n",
    "* Be able to extend the Monte Carlo algorithm to work with a state value function while maintaining a balance between exploration and exploitation.\n",
    "* Be able to implement the incremental Monte Carlo algorithm.\t\n",
    "* Be able to apply the Q-Learning algorithm from scratch.\n",
    "\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "Please group up in pairs and open the notebook from week 01 in either Google Colab or on your local machine.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "This graded lab is designed for you to prove your understanding of the learned methods. The tasks are threefold:\n",
    "\n",
    "\n",
    "\t\n",
    "* **1. Monte Carlo with State Value Function:** For the first task, you are required to take the **Monte Carlo algorithm** from the first week's lab (Grid World) and **modify it to work with a state-value function instead of a random choice**. This means that instead of randomly sampling from all possible actions, you sample according to the estimated state-value function. You should also **incorporate a strategy to balance exploration and exploitation using ε-Greedy**.\n",
    "\t\n",
    "* **2. Incremental Monte Carlo:** In the second task, extend the **Monte Carlo algorithm** from the first task **to an incremental Monte Carlo method**. Incremental Monte Carlo updates the value estimates incrementally, reducing the need to wait until the end of an episode to update values. This should lead to more efficient learning.\n",
    "\t\n",
    "* **3. Q-Learning Integration:** For the final task, you should **replace the Monte Carlo method** used and extended in the first two tasks **with the Q-Learning algorithm**, as introduced last week. This will allow you to compare the performance of Q-Learning with the Monte Carlo approach and observe how the learning process differs.\n",
    "\n",
    "\n",
    "### Submission\n",
    "\n",
    "* The deadline for completing this graded lab is the start of the lab session in week 7 (31st of October, 16:00). If you are attending the lecture in person, please be prepared to show your results upfront. If you are attending online, you should send an email to embe@zhaw.ch with your lab results attached. Make sure to include the names of both students if you are working in a pair.\n",
    "\n",
    "### Key Takeaways\n",
    "\t\n",
    "* **Advanced Techniques:** You will be applying advanced reinforcement learning techniques, including state value functions, incremental Monte Carlo, and Q-Learning.\n",
    "\t\n",
    "* **Efficient Learning:** Incremental Monte Carlo is designed to make learning more efficient by updating value estimates incrementally.\n",
    "\t\n",
    "* **Comparative Analysis:** By implementing Q-Learning for the same GridWorld environment, you will gain insights into the performance differences between the Monte Carlo method and Q-Learning in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b3c44b",
   "metadata": {},
   "source": [
    "## A1  Monte Carlo with State Value Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fce8215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9999\n",
      "-14.02 | -12.39 | -10.29 |  -8.23 |  -6.67 | \n",
      "-29.47 | -26.06 |  -9.47 |  -7.20 |  -5.77 | \n",
      "-38.42 | -15.34 |  -8.83 |  -6.02 |  -4.32 | \n",
      "-86.25 | -20.43 |  -7.67 |  -4.62 |  -2.99 | \n",
      "-79.43 | -45.21 |  -6.72 |  -3.41 |    @   | \n",
      "\n",
      "⮕ | ⮕ | ⮕ | ⮕ | ⬇ | \n",
      "⬆ | ⮕ | ⮕ | ⮕ | ⬇ | \n",
      "⬅ | ⮕ | ⮕ | ⮕ | ⬇ | \n",
      "⮕ | ⮕ | ⮕ | ⮕ | ⬇ | \n",
      "⬇ | ⬇ | ⮕ | ⮕ | @ | \n",
      "\n",
      "12768\n"
     ]
    }
   ],
   "source": [
    "from simple_grid_world import SimpleGridWorld\n",
    "from monte_carlo_generation_a1 import MonteCarloGeneration\n",
    "from monte_carlo_experiment_a1 import MonteCarloExperiment\n",
    "from visualize import state_value_2d, next_best_value_2d\n",
    "from IPython.display import clear_output\n",
    "\n",
    "env = SimpleGridWorld() # Instantiate the environment\n",
    "agent = MonteCarloExperiment(env=env)\n",
    "generator = MonteCarloGeneration(env=env, agent=agent) # Instantiate the trajectory generator\n",
    "for i in range(10000):\n",
    "  clear_output(wait=True)\n",
    "  generator.run_episode()\n",
    "print(state_value_2d(env, agent))\n",
    "print(next_best_value_2d(env, agent), flush=True)\n",
    "print(agent.random_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a51b08b",
   "metadata": {},
   "source": [
    "## A2 Incremental Monte Carlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d7a5cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9999\n",
      " -0.03 |  -0.16 |  -0.37 |  -0.45 |  -0.28 | \n",
      " -0.01 |  -0.07 |  -0.40 |  -0.27 |  -0.54 | \n",
      " -0.01 |  -0.07 |  -0.30 |  -0.10 |  -0.13 | \n",
      " -0.02 |  -0.07 |  -0.06 |  -0.04 |  -0.06 | \n",
      " -0.02 |  -0.01 |  -0.01 |  -0.01 |    @   | \n",
      "\n",
      "⬇ | ⬅ | ⬆ | ⬆ | ⬆ | \n",
      "⬇ | ⬅ | ⮕ | ⬇ | ⮕ | \n",
      "⬇ | ⬇ | ⮕ | ⬇ | ⬇ | \n",
      "⬇ | ⬇ | ⮕ | ⬇ | ⬇ | \n",
      "⮕ | ⮕ | ⮕ | ⮕ | @ | \n",
      "\n",
      "31079\n"
     ]
    }
   ],
   "source": [
    "from simple_grid_world import SimpleGridWorld\n",
    "from monte_carlo_generation_a2 import MonteCarloGeneration\n",
    "from monte_carlo_experiment_a2 import MonteCarloExperiment\n",
    "from visualize import state_value_2d, next_best_value_2d\n",
    "from IPython.display import clear_output\n",
    "\n",
    "env = SimpleGridWorld() # Instantiate the environment\n",
    "agent = MonteCarloExperiment(env=env)\n",
    "generator = MonteCarloGeneration(env=env, agent=agent) # Instantiate the trajectory generator\n",
    "for i in range(10000):\n",
    "  clear_output(wait=True)\n",
    "  generator.run_episode()\n",
    "print(state_value_2d(env, agent))\n",
    "print(next_best_value_2d(env, agent), flush=True)\n",
    "print(agent.random_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4613f250",
   "metadata": {},
   "source": [
    "## A3 Q-Learning Integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a4c97c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -6.32 |  -5.61 |  -4.66 |  -3.94 | \n",
      " -5.60 |  -4.83 |  -3.88 |  -3.21 | \n",
      " -4.52 |  -3.81 |  -2.91 |  -2.23 | \n",
      " -3.63 |  -3.01 |  -2.15 |    @   | \n",
      "\n",
      "⮕ | ⮕ | ⮕ | ⬇ | \n",
      "⮕ | ⮕ | ⮕ | ⬇ | \n",
      "⮕ | ⮕ | ⮕ | ⬇ | \n",
      "⮕ | ⮕ | ⮕ | @ | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from simple_grid_world import SimpleGridWorld\n",
    "from monte_carlo_generation_a3 import MonteCarloGeneration\n",
    "from monte_carlo_experiment_a3 import MonteCarloExperiment\n",
    "from visualize import state_value_2d, next_best_value_2d\n",
    "from IPython.display import clear_output\n",
    "\n",
    "env = SimpleGridWorld() # Instantiate the environment\n",
    "agent = MonteCarloExperiment(env=env)\n",
    "generator = MonteCarloGeneration(env=env, agent=agent) # Instantiate the trajectory generator\n",
    "for i in range(10000):\n",
    "  clear_output(wait=True)\n",
    "  generator.run_episode()\n",
    "print(state_value_2d(env, agent))\n",
    "print(next_best_value_2d(env, agent), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53af740",
   "metadata": {},
   "source": [
    "### Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1eaef7",
   "metadata": {},
   "source": [
    "|  | Monte Carlo | Incremental Monte Carlo |\n",
    "| --| --| --|\n",
    "| **First Visit**  | **1.** Epoche durchlaufen <br>  **2.** Backpropagate und alle values definieren $S(s) + G_t$ und Counter incrementieren <br> **3.** Von vorne beginnend: nimm erster state-value und dividiere durch Anzahl $N(S)$  |  **1.** Epoche durchlaufen <br> **2.** Backpropagate und alle values definieren $V(S_t) + \\frac{1}{N(S_t)} (G_t - V(S_t))$ und Counter incrementieren <br> **3.** Von vorne beginnend: nimm erster state-value;|\n",
    "| **Every-visit**  | **1.** Epoche durchlaufen; Backpropagate und alle values definieren $S(s) + G_t$ und Counter incrementieren <br> **2.** Von vorne beginnend: addiere (state-values dividiert durch Anzahl $N(S)$) | **1.** Epoche durchlaufen <br> **2.** Backpropagate und alle values definieren $V(S_t) + \\frac{1}{N(S_t)} (G_t - V(S_t))$ und aufsummieren und Counter incrementieren; |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e083c8b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37f5c3cd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
